{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "from keras.applications.imagenet_utils import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_CLASSES = 500\n",
    "IMG_SIZE = (512, 512)\n",
    "MAP_SIZE = (32, 32)\n",
    "DATASET_PATH = '/local/DEEPLEARNING/oid/'\n",
    "BATCH_SIZE = 16\n",
    "SEED = 1\n",
    "\n",
    "# overlaps for RPN\n",
    "RPN_MIN_OVERLAP = 0.3\n",
    "RPN_MAX_OVERLAP = 0.7\n",
    "\n",
    "NUM_REGIONS = 256    # how many anchors we keep for one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "RPN_STRIDE = 16\n",
    "ANCHOR_SCALES = [64, 128, 256] \n",
    "ANCHOR_RATIOS = [[1, 1], [1./math.sqrt(2), 2./math.sqrt(2)], [2./math.sqrt(2), 1./math.sqrt(2)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annotations(annotations_path):\n",
    "        '''\n",
    "        one annotation csv line is like:\n",
    "        img_id,folder,class_id,xmin,ymin,xmax,ymax\n",
    "\n",
    "        38a420b38cd3c350,train_3,92,0.325781,0.450781,0.447917,0.668750\n",
    "\n",
    "        ---\n",
    "\n",
    "        load annotations:\n",
    "        - samples[img_id][multilabel] = list(one_hot)   # (N, nb_classes) list of variable size depending on the number of bounding boxes that are GT\n",
    "        - samples[img_id][bboxes] = list(bbox)          # (N, 4) list of variable size of 4 int tuples representing x1, y1, x2, y2\n",
    "\n",
    "        example:\n",
    "        samples['5c015f7e9bbd728a']['multilabel'][0] = [0, 1, ....] (length 500)\n",
    "        samples['5c015f7e9bbd728a']['bboxes'][0] = (x1, y1, x2, y2)\n",
    "        '''\n",
    "\n",
    "        samples = defaultdict(lambda: defaultdict(list))\n",
    "        class_data = load_class_data()\n",
    "\n",
    "        # multilabel annotations\n",
    "        with open(annotations_path, 'r') as f_in:\n",
    "            for line in f_in:\n",
    "                parts = line.strip().split(',')\n",
    "                img_id = parts[0]\n",
    "                class_id = int(parts[2])\n",
    "                ground_truth_cls = one_hotify_gt(class_id)\n",
    "\n",
    "                samples[img_id]['multilabel'].append(ground_truth_cls)\n",
    "                samples[img_id]['bboxes'].append((float(parts[3]), float(parts[4]), float(parts[5]), float(parts[6])))\n",
    "                samples[img_id]['classes'].append((class_data[class_id]['id'], class_data[class_id]['name']))\n",
    "                \n",
    "                size = (parts[7], parts[8])\n",
    "                \n",
    "                if samples[img_id]['size'] == list(): # default \n",
    "                    samples[img_id]['size'] = size \n",
    "                else:\n",
    "                    assert samples[img_id]['size'] == size    # it should always be the same\n",
    "\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_class_data():\n",
    "    class_data = dict()\n",
    "    class_data_path = os.path.join(DATASET_PATH, 'annotations', 'challenge-2018-classes.csv')\n",
    "    \n",
    "    with open(class_data_path, 'r') as f_in:\n",
    "        for line in f_in:\n",
    "            parts = line.strip().split(',')\n",
    "            class_id = int(parts[0])\n",
    "            class_name = parts[1]\n",
    "            class_oid = parts[2]\n",
    "            \n",
    "            class_data[class_id] = {'id': class_id, 'name': class_name, 'oid': class_oid}\n",
    "            \n",
    "    return class_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hotify_gt(numeric_gt):\n",
    "    return np.array([int(i == numeric_gt) for i in range(NB_CLASSES)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded annotations in 2573.7738497257233\n"
     ]
    }
   ],
   "source": [
    "annotations_file = os.path.join(DATASET_PATH, 'annotations', 'challenge-2018-train.csv')\n",
    "\n",
    "t0 = time.time()\n",
    "samples = load_annotations(annotations_file)\n",
    "t1 = time.time()\n",
    "\n",
    "total = t1-t0\n",
    "print('loaded annotations in %s' % total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1024', '678')\n",
      "('1024', '678')\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-2095d0e15b18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_annotations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotations_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-58-350eafccde87>\u001b[0m in \u001b[0;36mload_annotations\u001b[0;34m(annotations_path)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "test = load_annotations(annotations_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1578819"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids = sorted(samples.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226838"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ids.index('1cb71505057b99dc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_samples = len(sample_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del samples[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ints = list()\n",
    "for sid in samples.keys():\n",
    "    if isinstance(sid, int):\n",
    "        list_ints.append(sid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_ints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0.325781', '0.447917', '0.450781', '0.668750'),\n",
       " ('0.279687', '0.339583', '0.344531', '0.501042'),\n",
       " ('0.307031', '0.240625', '0.401563', '0.398958'),\n",
       " ('0.373437', '0.492708', '0.447656', '0.657292'),\n",
       " ('0.496875', '0.221875', '0.600000', '0.425000'),\n",
       " ('0.073437', '0.323958', '0.244531', '0.735417'),\n",
       " ('0.121875', '0.508333', '0.283594', '0.831250'),\n",
       " ('0.132031', '0.493750', '0.392969', '0.769792'),\n",
       " ('0.399219', '0.321875', '0.675000', '0.469792'),\n",
       " ('0.378125', '0.509375', '0.445312', '0.582292'),\n",
       " ('0.314063', '0.401042', '0.340625', '0.439583'),\n",
       " ('0.364062', '0.281250', '0.388281', '0.328125'),\n",
       " ('0.403125', '0.555208', '0.430469', '0.585417'),\n",
       " ('0.568750', '0.278125', '0.592969', '0.326042'),\n",
       " ('0.094531', '0.942708', '0.146094', '0.998958'),\n",
       " ('0.128125', '0.528125', '0.271094', '0.640625'),\n",
       " ('0.153906', '0.619792', '0.247656', '0.747917'),\n",
       " ('0.591406', '0.320833', '0.672656', '0.397917'),\n",
       " ('0.648438', '0.354167', '0.796875', '0.864583'),\n",
       " ('0.440625', '0.187500', '0.591406', '0.442708'),\n",
       " ('0.452344', '0.186458', '0.607031', '0.403125'),\n",
       " ('0.696875', '0.410417', '0.800781', '0.857292'),\n",
       " ('0.305469', '0.376042', '0.325000', '0.404167'),\n",
       " ('0.332031', '0.390625', '0.342187', '0.414583'),\n",
       " ('0.335156', '0.277083', '0.367969', '0.292708'),\n",
       " ('0.387500', '0.273958', '0.399219', '0.294792'),\n",
       " ('0.544531', '0.294792', '0.562500', '0.308333'),\n",
       " ('0.572656', '0.271875', '0.591406', '0.289583'),\n",
       " ('0.288281', '0.422917', '0.326562', '0.462500'),\n",
       " ('0.350000', '0.327083', '0.385938', '0.360417'),\n",
       " ('0.396094', '0.581250', '0.435937', '0.619792'),\n",
       " ('0.559375', '0.321875', '0.598437', '0.378125'),\n",
       " ('0.493750', '0.848958', '0.650000', '0.998958'),\n",
       " ('0.557031', '0.742708', '0.824219', '0.998958'),\n",
       " ('0.777344', '0.647917', '0.947656', '0.994792'),\n",
       " ('0.475781', '0.322917', '0.510156', '0.371875'),\n",
       " ('0.000000', '0.153125', '0.677344', '0.998958'),\n",
       " ('0.087500', '0.453125', '0.696875', '0.998958'),\n",
       " ('0.101562', '0.267708', '0.453125', '0.996875'),\n",
       " ('0.190625', '0.263542', '0.358594', '0.498958'),\n",
       " ('0.253125', '0.159375', '0.416406', '0.289583'),\n",
       " ('0.328906', '0.443750', '0.428906', '0.732292'),\n",
       " ('0.192969', '0.270833', '0.361719', '0.491667'),\n",
       " ('0.251563', '0.161458', '0.417969', '0.395833')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples['38a420b38cd3c350']['bboxes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def union(au, bu, area_intersection):\n",
    "\tarea_a = (au[2] - au[0]) * (au[3] - au[1])\n",
    "\tarea_b = (bu[2] - bu[0]) * (bu[3] - bu[1])\n",
    "\tarea_union = area_a + area_b - area_intersection\n",
    "\treturn area_union\n",
    "\n",
    "\n",
    "def intersection(ai, bi):\n",
    "\tx = max(ai[0], bi[0])\n",
    "\ty = max(ai[1], bi[1])\n",
    "\tw = min(ai[2], bi[2]) - x\n",
    "\th = min(ai[3], bi[3]) - y\n",
    "\tif w < 0 or h < 0:\n",
    "\t\treturn 0\n",
    "\treturn w*h\n",
    "\n",
    "\n",
    "def compute_iou(a, b):\n",
    "\t# a and b should be (x1,y1,x2,y2)\n",
    "\n",
    "\tif a[0] >= a[2] or a[1] >= a[3] or b[0] >= b[2] or b[1] >= b[3]:\n",
    "\t\treturn 0.0\n",
    "\n",
    "\tarea_i = intersection(a, b)\n",
    "\tarea_u = union(a, b, area_i)\n",
    "\n",
    "\treturn float(area_i) / float(area_u + 1e-6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rpn(img_data):\n",
    "    '''\n",
    "    getting the RPN ground truth for all anchors (9 anchors total)\n",
    "    \n",
    "    if the CNN output size is 56x56=3136, there are 3136x9=28224 potential anchors\n",
    "    we select 256 of them, balanced between positive and negative anchors\n",
    "    \n",
    "    inputs: \n",
    "        - img_data: the dict containing information about the image for which the RPN ground truth is computed (= samples[img_id])\n",
    "\n",
    "\tReturns:\n",
    "\t\ty_rpn_cls: list(num_bboxes, y_is_box_valid + y_rpn_overlap)\n",
    "\t\t\ty_is_box_valid: 0 or 1 (0 means the box is invalid, 1 means the box is valid)\n",
    "\t\t\ty_rpn_overlap: 0 or 1 (0 means the box is not an object, 1 means the box is an object)\n",
    "\t\ty_rpn_regr: list(num_bboxes, 4*y_rpn_overlap + y_rpn_regr)\n",
    "\t\t\ty_rpn_regr: x1,y1,x2,y2 bunding boxes coordinates\n",
    "    '''\n",
    "    \n",
    "    downscale = float(RPN_STRIDE) \n",
    "    anchor_sizes = ANCHOR_SCALES   # [64, 128, 256] because the input image is half the original size so we take half the original anchors\n",
    "    anchor_ratios = ANCHOR_RATIOS  # 1:1, 1:2*sqrt(2), 2*sqrt(2):1\n",
    "    num_anchors = len(anchor_sizes) * len(anchor_ratios) # 3x3=9\n",
    "    \n",
    "    original_width = img_data['size'][0]\n",
    "    original_height = img_data['size'][1]\n",
    "    print(original_width)\n",
    "    print(original_height)\n",
    "    print(img_data['size'])\n",
    "    resized_width, resized_height = IMG_SIZE\n",
    "    \n",
    "    map_width, map_height = MAP_SIZE  # = output_width, output_height\n",
    "    \n",
    "    downscale_x = resized_width / map_width\n",
    "    downscale_y = resized_height / map_height\n",
    "    \n",
    "    # initialise empty output batch\n",
    "    y_rpn_overlap = np.zeros((map_height, map_width, num_anchors))      # one bool for each anchor. 1 means anchor overlaps with GT and 0 not\n",
    "    y_is_box_valid = np.zeros((map_height, map_width, num_anchors))     # one bool for each anchor for validity (i.e. whether the box is clearly negative of clearly positive)\n",
    "    y_rpn_regr = np.zeros((map_height, map_width, num_anchors * 4))     # regression coordinates for each anchor\n",
    "    \n",
    "    # ground truth bounding boxes for the image\n",
    "    num_bboxes = len(img_data['bboxes'])\n",
    "\n",
    "    num_anchors_for_bbox = np.zeros(num_bboxes).astype(int)             # will store how many anchor boxes match the ground truth bbox\n",
    "    best_anchor_for_bbox = -1 * np.ones((num_bboxes, 4)).astype(int)    # keep track of which anchor is best for each gt box. anchor is described as (x, y, i_anchor_size, i_anchor_ratio). x and y are the coords in the cnn output feature map from which the anchor is\n",
    "    best_iou_for_bbox = np.zeros(num_bboxes).astype(np.float32)         # iou between best anchor box and gt box\n",
    "    best_anchor_bbox_for_bbox = np.zeros((num_bboxes, 4)).astype(int)           # bounding box of the best anchor for the gt box\n",
    "    best_anchor_var_for_bbox = np.zeros((num_bboxes, 4)).astype(np.float32)     # variation\n",
    "    \n",
    "    # get the GT box coordinates, and resize to account for image resizing\n",
    "    gt_boxes = np.zeros((num_bboxes, 4))    # xmin, ymin, xmax, ymax\n",
    "    for i_bbox, bbox in enumerate(img_data['bboxes']):\n",
    "        \n",
    "        # get the GT box coordinates, and resize to account for image resizing\n",
    "        # TODO: remove the float when data are re-loaded\n",
    "        # gt_boxes[i_bbox, 0] = float(bbox[0]) * (resized_width / float(original_width))      # xmin\n",
    "        # gt_boxes[i_bbox, 1] = float(bbox[1]) * (resized_height / float(original_height))    # ymin\n",
    "        # gt_boxes[i_bbox, 2] = float(bbox[2]) * (resized_width / float(original_width))      # xmax\n",
    "        # gt_boxes[i_bbox, 3] = float(bbox[3]) * (resized_height / float(original_height))    # ymax\n",
    "        gt_boxes[i_bbox, 0] = float(bbox[0]) * resized_width      # xmin\n",
    "        gt_boxes[i_bbox, 1] = float(bbox[1]) * resized_height     # ymin\n",
    "        gt_boxes[i_bbox, 2] = float(bbox[2]) * resized_width      # xmax\n",
    "        gt_boxes[i_bbox, 3] = float(bbox[3]) * resized_height     # ymax\n",
    "        \n",
    "    print('image has %s GT boxes' % len(img_data['bboxes']))\n",
    "    # --------------- rpn ground truth -----------------\n",
    "    \n",
    "    # double loop to iterate on each anchor type (size + ratio) (9 total)\n",
    "    for i_anchor_size in range(len(anchor_sizes)):\n",
    "        for i_anchor_ratio in range(len(anchor_ratios)):\n",
    "            anchor_width = anchor_sizes[i_anchor_size] * anchor_ratios[i_anchor_ratio][0]\n",
    "            anchor_height = anchor_sizes[i_anchor_size] * anchor_ratios[i_anchor_ratio][1]\n",
    "            \n",
    "            # x-coordinates of the current anchor box\n",
    "            for ix in range(map_width):                     \n",
    "                x1_anc = downscale_x * (ix + 0.5) - anchor_width / 2\n",
    "                x2_anc = downscale_x * (ix + 0.5) + anchor_width / 2 \n",
    "                \n",
    "                # ignore boxes that go across image boundaries                  \n",
    "                if x1_anc < 0 or x2_anc > resized_width:\n",
    "                    continue\n",
    "                    \n",
    "                # y-coordinates of the current anchor box\n",
    "                for jy in range(map_height):\n",
    "                    y1_anc = downscale_y * (jy + 0.5) - anchor_height / 2\n",
    "                    y2_anc = downscale_y * (jy + 0.5) + anchor_height / 2\n",
    "                    \n",
    "                    # ignore boxes that go across image boundaries\n",
    "                    if y1_anc < 0 or y2_anc > resized_height:\n",
    "                        continue\n",
    "\n",
    "                    # bbox_type indicates whether an anchor should be a target (default is negative)\n",
    "                    anchor_box_type = 'neg'\n",
    "\n",
    "                    # this is the best IOU for the (x,y) coord and the current anchor\n",
    "                    # note that this is different from the best IOU for a GT bbox\n",
    "                    # TODO: ???\n",
    "                    best_iou_for_anchor = 0.0\n",
    "                    best_regr_for_anchor = None\n",
    "                    \n",
    "                    # for each ground truth bbox \n",
    "                    for i_gt_box in range(num_bboxes):\n",
    "                        # get IoU between current GT box and the current anchor box\n",
    "                        curr_iou = compute_iou([gt_boxes[i_gt_box, 0], gt_boxes[i_gt_box, 1], gt_boxes[i_gt_box, 2], gt_boxes[i_gt_box, 3]], [x1_anc, y1_anc, x2_anc, y2_anc])\n",
    "                        if curr_iou > 0.5:\n",
    "                            print('gt box [%s %s %s %s], anchor [%s %s %s %s]' % (gt_boxes[i_gt_box, 0], gt_boxes[i_gt_box, 1], gt_boxes[i_gt_box, 2], gt_boxes[i_gt_box, 3], x1_anc, y1_anc, x2_anc, y2_anc))\n",
    "                            print('iou for anchor [%s %s %s %s] and bbox %s : %s' % (i_anchor_size, i_anchor_ratio, ix, jy, i_gt_box, curr_iou))\n",
    "                        \n",
    "                        # calculate the regression targets if needed (best_iou_for_bbox is init to 0.0)\n",
    "                        if curr_iou > best_iou_for_bbox[i_gt_box] or curr_iou > RPN_MAX_OVERLAP:\n",
    "                            \n",
    "                            # center points of GT box and anchor box\n",
    "                            gt_center_x = (gt_boxes[i_gt_box, 0] + gt_boxes[i_gt_box, 2]) / 2.0\n",
    "                            gt_center_y = (gt_boxes[i_gt_box, 1] + gt_boxes[i_gt_box, 3]) / 2.0\n",
    "                            anc_center_x = (x1_anc + x2_anc) / 2.0\n",
    "                            anc_center_y = (y1_anc + y2_anc) / 2.0\n",
    "                            \n",
    "                            anchor_width = x2_anc - x1_anc\n",
    "                            anchor_heigth = y2_anc - y1_anc\n",
    "                            \n",
    "                            gt_width = gt_boxes[i_gt_box, 2] - gt_boxes[i_gt_box, 0]\n",
    "                            gt_height = gt_boxes[i_gt_box, 3] - gt_boxes[i_gt_box, 1]\n",
    "                            \n",
    "                            # target variations to transform the anchor box into the gt box\n",
    "                            # this is calculated in the image reference (not the map)\n",
    "                            target_dx = (gt_center_x - anc_center_x) / anchor_width\n",
    "                            target_dy = (gt_center_y - anc_center_y) / anchor_heigth\n",
    "                            target_dw = np.log(gt_width / anchor_width)\n",
    "                            target_dh = np.log(gt_height / anchor_height)\n",
    "                        \n",
    "                        # keep track of which anchor box was best for current GT box\n",
    "                        if curr_iou > best_iou_for_bbox[i_gt_box]:\n",
    "                            best_iou_for_bbox[i_gt_box] = curr_iou\n",
    "                            best_anchor_for_bbox[i_gt_box] = [ix, jy, i_anchor_size, i_anchor_ratio]\n",
    "                            best_anchor_bbox_for_bbox[i_gt_box,:] = [x1_anc, y1_anc, x2_anc, y2_anc]\n",
    "                            best_anchor_var_for_bbox[i_gt_box,:] = [target_dx, target_dy, target_dw, target_dh]\n",
    "                            \n",
    "                        # we set the anchor to positive if the IOU is >0.7 (it does not matter if there was another better box, it just indicates overlap)\n",
    "                        if curr_iou > RPN_MAX_OVERLAP:\n",
    "                            print('found positive anchor for gt box %s' % i_gt_box)\n",
    "                            anchor_box_type = 'pos'\n",
    "                            num_anchors_for_bbox[i_gt_box] += 1\n",
    "                            # we update the regression layer target if this IOU is the best for the current (x,y) and anchor position\n",
    "                            if curr_iou > best_iou_for_anchor:\n",
    "                                best_iou_for_anchor = curr_iou\n",
    "                                best_regr_for_anchor = (target_dx, target_dy, target_dw, target_dh)\n",
    "                                \n",
    "                        # if the IOU is >0.3 and <0.7, it is ambiguous and no included in the objective\n",
    "                        if RPN_MIN_OVERLAP < curr_iou < RPN_MAX_OVERLAP:\n",
    "                            # gray zone between neg and pos\n",
    "                            if anchor_box_type == 'neg':\n",
    "                                anchor_box_type = 'neutral'\n",
    "                            \n",
    "                    # turn on or off outputs depending on IOUs\n",
    "                    anchor_index = i_anchor_ratio + len(anchor_ratios) * i_anchor_size   # 0: 64, 1x1; 1: 64, 1x2V2; 2: 64, 2V2x1; 3: 128, 1x1, etc til 8 (= 9 anchors)\n",
    "                    if anchor_box_type == 'neg':\n",
    "                        y_is_box_valid[ix, jy, anchor_index] = 1\n",
    "                        y_rpn_overlap[ix, jy, anchor_index] = 0\n",
    "                    elif anchor_box_type == 'neutral':\n",
    "                        y_is_box_valid[ix, jy, anchor_index] = 0\n",
    "                        y_rpn_overlap[ix, jy, anchor_index] = 0\n",
    "                    elif anchor_box_type == 'pos':\n",
    "                        y_is_box_valid[ix, jy, anchor_index] = 1\n",
    "                        y_rpn_overlap[ix, jy, anchor_index] = 1\n",
    "                        start = 4 * anchor_index\n",
    "                        y_rpn_regr[ix, jy, start:start+4] = best_regr_for_anchor\n",
    "                        \n",
    "    # find a ok-ish anchor for ground truth that don't have good anchors\n",
    "    print(num_anchors_for_bbox)\n",
    "    for i_gt, nb_anchors in enumerate(num_anchors_for_bbox):\n",
    "        print('we found %s anchors for %s' % (nb_anchors, i_gt))\n",
    "        if nb_anchors == 0:\n",
    "            print('we didnt find any anchor for gt %s' % i_gt)\n",
    "            # we didn't find any box with iou > 0 with GT...\n",
    "            if best_anchor_for_bbox[i_gt, 0] == -1:\n",
    "                print('hopeless for gt %s' % i_gt)\n",
    "                continue\n",
    "                \n",
    "            best_anchor_mapx, best_anchor_mapy, best_anchor_size_i, best_anchor_ratio_i = best_anchor_for_bbox[i_gt]\n",
    "            anchor_index = best_anchor_ratio_i + len(anchor_ratios) * best_anchor_size_i\n",
    "            \n",
    "            y_is_box_valid[best_anchor_mapx, best_anchor_mapy, anchor_index] = 1\n",
    "            y_rpn_overlap[best_anchor_mapx, best_anchor_mapy, anchor_index] = 1\n",
    "            start = 4 * anchor_index\n",
    "            y_rpn_regr[best_anchor_mapx, best_anchor_mapy, start:start+4] = best_anchor_var_for_bbox[i_gt, :]\n",
    "            \n",
    "    # for outputs change dims to (anchor index (0-8), anchor_mapx, anchor_mapy) and add first dimension\n",
    "    y_rpn_overlap = np.transpose(y_rpn_overlap, (2, 0, 1))\n",
    "    y_rpn_overlap = np.expand_dims(y_rpn_overlap, axis=0)\n",
    "    \n",
    "    y_is_box_valid = np.transpose(y_is_box_valid, (2, 0, 1))\n",
    "    y_is_box_valid = np.expand_dims(y_is_box_valid, axis=0)\n",
    "    \n",
    "    y_rpn_regr = np.transpose(y_rpn_regr, (2, 0, 1))\n",
    "    y_rpn_regr = np.expand_dims(y_rpn_regr, axis=0)\n",
    "    \n",
    "    # valid and overlap = positive, valid and not overlap = negative\n",
    "    pos_locs = np.where(np.logical_and(y_rpn_overlap[0, :, :, :] == 1, y_is_box_valid[0, :, :, :] == 1))\n",
    "    neg_locs = np.where(np.logical_and(y_rpn_overlap[0, :, :, :] == 0, y_is_box_valid[0, :, :, :] == 1))\n",
    "    \n",
    "    assert len(pos_locs[0]) == len(pos_locs[1]) == len(pos_locs[2])\n",
    "    nb_pos = len(pos_locs[0])\n",
    "    nb_neg = len(neg_locs[0])\n",
    "    print('found %s positives and %s negatives' % (nb_pos, nb_neg))\n",
    "    \n",
    "    # one issue is that the RPN has many more negative than positive regions, so we turn off some of the negative\n",
    "    # regions. We also limit it to 256 regions.\n",
    "    num_regions = NUM_REGIONS\n",
    "\n",
    "    # in case we have more than half the budget of positive anchors we invalidate some of them\n",
    "    if nb_pos > num_regions / 2:\n",
    "        val_locs = random.sample(range(nb_pos), nb_pos - num_regions/2)\n",
    "        y_is_box_valid[0, pos_locs[0][val_locs], pos_locs[1][val_locs], pos_locs[2][val_locs]] = 0\n",
    "        nb_pos = num_regions / 2\n",
    "\n",
    "    # invalidate excess negative examples so we have the same number of negatives and positives (max NUM_REGIONS for both)\n",
    "    if nb_neg + nb_pos > num_regions:\n",
    "        val_locs = random.sample(range(nb_neg), nb_neg - nb_pos)\n",
    "        y_is_box_valid[0, neg_locs[0][val_locs], neg_locs[1][val_locs], neg_locs[2][val_locs]] = 0\n",
    "        \n",
    "    print('shape y_is_box_valid %s, nonzero %s' % (str(y_is_box_valid.shape), np.count_nonzero(y_is_box_valid)))\n",
    "    print('shape y_rpn_overlap %s, nonzero %s' % (str(y_rpn_overlap.shape), np.count_nonzero(y_rpn_overlap)))\n",
    "    print('shape y_rpn_regr %s' % str(y_rpn_regr.shape))\n",
    "    \n",
    "    y_rpn_cls = np.concatenate([y_is_box_valid, y_rpn_overlap], axis=1)\n",
    "    y_rpn_regr = np.concatenate([np.repeat(y_rpn_overlap, 4, axis=1), y_rpn_regr], axis=1)\n",
    "\n",
    "    return np.copy(y_rpn_cls), np.copy(y_rpn_regr), nb_pos\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# equivalent get_anchor_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_dict(samples, sample_ids, sample_idxs):\n",
    "    '''\n",
    "    Creation of the actual batch\n",
    "    '''\n",
    "    output = {}\n",
    "    sample_ids = [sample_ids[i] for i in sample_idxs]\n",
    "\n",
    "    img_batch = []\n",
    "    cls_batch = []\n",
    "    regr_batch = []\n",
    "\n",
    "    img_data_batch = []\n",
    "    ids_batch = []\n",
    "\n",
    "    for img_id in sample_ids:\n",
    "        img_data = dict()\n",
    "\n",
    "        img_path = os.path.join(DATASET_PATH, 'images', 'train', 'train_%s' % img_id[0], '%s.jpg' % img_id)\n",
    "        img = image.load_img(img_path, grayscale=False, target_size=(IMG_SIZE[0], IMG_SIZE[1]))\n",
    "        img_arr = image.img_to_array(img, data_format='channels_last')\n",
    "        \n",
    "        img_batch.append(img_arr)\n",
    "        \n",
    "        y_rpn_cls, y_rpn_regr, num_pos = calc_rpn(samples[img_id])\n",
    "        print(\"shape rpn cls %s\" % str(y_rpn_cls.shape))\n",
    "        print(\"shape rpn regr %s\" % str(y_rpn_regr.shape))\n",
    "        print('num pos %s' % num_pos)\n",
    "        \n",
    "        # y_rpn_regr[:, y_rpn_regr.shape[1]//2:, :, :] *= C.std_scaling\n",
    "        y_rpn_regr = np.transpose(y_rpn_regr, (0, 2, 3, 1))\n",
    "        y_rpn_cls = np.transpose(y_rpn_cls, (0, 2, 3, 1))\n",
    "        \n",
    "        cls_batch.append(y_rpn_cls)\n",
    "        regr_batch.append(y_rpn_regr)\n",
    "        ids_batch.append(img_id)\n",
    "        \n",
    "        # TODO: data augmentation\n",
    "        \n",
    "        img_data['num_pos'] = num_pos\n",
    "        img_data['original_size'] = (samples[img_id]['size'][0], samples[img_id]['size'][1])\n",
    "        img_data['bboxes'] = samples[img_id]['bboxes']\n",
    "        img_data['classes'] = samples[img_id]['classes']\n",
    "\n",
    "        img_data_batch.append(img_data)\n",
    "\n",
    "    img_batch = np.reshape(img_batch, (-1, IMG_SIZE[0], IMG_SIZE[1], 3))  # TODO: figure out why this line is necessary\n",
    "    img_batch = preprocess_input(img_batch, data_format='channels_last')\n",
    "\n",
    "    ids_batch = np.reshape(np.array(ids_batch), (-1, 1))\n",
    "\n",
    "    output['image'] = img_batch\n",
    "    output['rpn_cls'] = cls_batch\n",
    "    output['rpn_regr'] = regr_batch\n",
    "    output['img_data'] = img_data_batch\n",
    "    output['image_id'] = ids_batch\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sample idx for the batch\n",
    "def get_batch_indexes(batch_idx):\n",
    "\n",
    "    batch_idxs = list()\n",
    "    for i in range(BATCH_SIZE):\n",
    "        sample_idx = batch_idx * BATCH_SIZE + i\n",
    "        if sample_idx >= nb_samples:\n",
    "            sample_idx -= nb_samples\n",
    "        batch_idxs.append(sample_idx)\n",
    "    return batch_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n"
     ]
    }
   ],
   "source": [
    "batch_idxs = get_batch_indexes(4)\n",
    "print(batch_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_idxs = [226838]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n",
      "768\n",
      "('1024', '768')\n",
      "image has 4 GT boxes\n",
      "gt box [92.8 255.573504 139.52 310.613504], anchor [72.0 248.0 136.0 312.0]\n",
      "iou for anchor [0 0 6 17] and bbox 0 : 0.5542824404322323\n",
      "gt box [92.8 255.573504 139.52 310.613504], anchor [88.0 248.0 152.0 312.0]\n",
      "iou for anchor [0 0 7 17] and bbox 0 : 0.6277999998467284\n",
      "gt box [332.16 251.306496 378.24 310.613504], anchor [312.0 248.0 376.0 312.0]\n",
      "iou for anchor [0 0 21 17] and bbox 2 : 0.61482924329103\n",
      "gt box [332.16 251.306496 378.24 310.613504], anchor [328.0 248.0 392.0 312.0]\n",
      "iou for anchor [0 0 22 17] and bbox 2 : 0.6672038398371078\n",
      "gt box [92.8 255.573504 139.52 310.613504], anchor [97.37258300203048 218.74516600406096 142.62741699796953 309.25483399593907]\n",
      "iou for anchor [0 1 7 16] and bbox 0 : 0.5136346257130668\n",
      "gt box [92.8 255.573504 139.52 310.613504], anchor [97.37258300203048 234.74516600406096 142.62741699796953 325.25483399593907]\n",
      "iou for anchor [0 1 7 17] and bbox 0 : 0.5335711266087212\n",
      "gt box [92.8 255.573504 139.52 310.613504], anchor [97.37258300203048 250.74516600406096 142.62741699796953 341.25483399593907]\n",
      "iou for anchor [0 1 7 18] and bbox 0 : 0.5335711266087212\n",
      "gt box [332.16 251.306496 378.24 310.613504], anchor [337.37258300203047 218.74516600406096 382.62741699796953 309.25483399593907]\n",
      "iou for anchor [0 1 22 16] and bbox 2 : 0.5309067776134292\n",
      "gt box [332.16 251.306496 378.24 310.613504], anchor [337.37258300203047 234.74516600406096 382.62741699796953 325.25483399593907]\n",
      "iou for anchor [0 1 22 17] and bbox 2 : 0.5502033396775539\n",
      "gt box [332.16 251.306496 378.24 310.613504], anchor [337.37258300203047 250.74516600406096 382.62741699796953 341.25483399593907]\n",
      "iou for anchor [0 1 22 18] and bbox 2 : 0.5502033396775539\n",
      "gt box [40.32 194.986496 390.72 306.773504], anchor [2.98066401624385 125.49033200812192 365.01933598375615 306.5096679918781]\n",
      "iou for anchor [2 2 11 13] and bbox 1 : 0.5286761889993655\n",
      "gt box [40.32 194.986496 390.72 306.773504], anchor [2.98066401624385 141.49033200812192 365.01933598375615 322.5096679918781]\n",
      "iou for anchor [2 2 11 14] and bbox 1 : 0.5305905230520659\n",
      "gt box [40.32 194.986496 390.72 306.773504], anchor [2.98066401624385 157.49033200812192 365.01933598375615 338.5096679918781]\n",
      "iou for anchor [2 2 11 15] and bbox 1 : 0.5305905230520659\n",
      "gt box [40.32 194.986496 390.72 306.773504], anchor [2.98066401624385 173.49033200812192 365.01933598375615 354.5096679918781]\n",
      "iou for anchor [2 2 11 16] and bbox 1 : 0.5305905230520659\n",
      "gt box [40.32 194.986496 390.72 306.773504], anchor [2.98066401624385 189.49033200812192 365.01933598375615 370.5096679918781]\n",
      "iou for anchor [2 2 11 17] and bbox 1 : 0.5305905230520659\n",
      "gt box [40.32 194.986496 390.72 306.773504], anchor [18.98066401624385 125.49033200812192 381.01933598375615 306.5096679918781]\n",
      "iou for anchor [2 2 12 13] and bbox 1 : 0.5695653051484934\n",
      "gt box [40.32 194.986496 390.72 306.773504], anchor [18.98066401624385 141.49033200812192 381.01933598375615 322.5096679918781]\n",
      "iou for anchor [2 2 12 14] and bbox 1 : 0.5716830686308414\n",
      "gt box [40.32 194.986496 390.72 306.773504], anchor [18.98066401624385 157.49033200812192 381.01933598375615 338.5096679918781]\n",
      "iou for anchor [2 2 12 15] and bbox 1 : 0.5716830686308414\n",
      "gt box [40.32 194.986496 390.72 306.773504], anchor [18.98066401624385 173.49033200812192 381.01933598375615 354.5096679918781]\n",
      "iou for anchor [2 2 12 16] and bbox 1 : 0.5716830686308414\n",
      "gt box [40.32 194.986496 390.72 306.773504], anchor [18.98066401624385 189.49033200812192 381.01933598375615 370.5096679918781]\n",
      "iou for anchor [2 2 12 17] and bbox 1 : 0.5716830686308414\n",
      "gt box [40.32 194.986496 390.72 306.773504], anchor [34.98066401624385 125.49033200812192 397.01933598375615 306.5096679918781]\n",
      "iou for anchor [2 2 13 13] and bbox 1 : 0.5954387247377405\n",
      "gt box [40.32 194.986496 390.72 306.773504], anchor [34.98066401624385 141.49033200812192 397.01933598375615 322.5096679918781]\n",
      "iou for anchor [2 2 13 14] and bbox 1 : 0.5976893249908803\n",
      "gt box [40.32 194.986496 390.72 306.773504], anchor [34.98066401624385 157.49033200812192 397.01933598375615 338.5096679918781]\n",
      "iou for anchor [2 2 13 15] and bbox 1 : 0.5976893249908803\n",
      "gt box [40.32 194.986496 390.72 306.773504], anchor [34.98066401624385 173.49033200812192 397.01933598375615 354.5096679918781]\n",
      "iou for anchor [2 2 13 16] and bbox 1 : 0.5976893249908803\n",
      "gt box [40.32 194.986496 390.72 306.773504], anchor [34.98066401624385 189.49033200812192 397.01933598375615 370.5096679918781]\n",
      "iou for anchor [2 2 13 17] and bbox 1 : 0.5976893249908803\n",
      "gt box [40.32 194.986496 390.72 306.773504], anchor [34.98066401624385 205.49033200812192 397.01933598375615 386.5096679918781]\n",
      "iou for anchor [2 2 13 18] and bbox 1 : 0.5127332476332653\n",
      "gt box [40.32 194.986496 390.72 306.773504], anchor [50.98066401624385 125.49033200812192 413.01933598375615 306.5096679918781]\n",
      "iou for anchor [2 2 14 13] and bbox 1 : 0.567050372091888\n",
      "gt box [40.32 194.986496 390.72 306.773504], anchor [50.98066401624385 141.49033200812192 413.01933598375615 322.5096679918781]\n",
      "iou for anchor [2 2 14 14] and bbox 1 : 0.5691553936493523\n",
      "gt box [40.32 194.986496 390.72 306.773504], anchor [50.98066401624385 157.49033200812192 413.01933598375615 338.5096679918781]\n",
      "iou for anchor [2 2 14 15] and bbox 1 : 0.5691553936493523\n",
      "gt box [40.32 194.986496 390.72 306.773504], anchor [50.98066401624385 173.49033200812192 413.01933598375615 354.5096679918781]\n",
      "iou for anchor [2 2 14 16] and bbox 1 : 0.5691553936493523\n",
      "gt box [40.32 194.986496 390.72 306.773504], anchor [50.98066401624385 189.49033200812192 413.01933598375615 370.5096679918781]\n",
      "iou for anchor [2 2 14 17] and bbox 1 : 0.5691553936493523\n",
      "gt box [40.32 194.986496 390.72 306.773504], anchor [66.98066401624385 125.49033200812192 429.01933598375615 306.5096679918781]\n",
      "iou for anchor [2 2 15 13] and bbox 1 : 0.526290483785415\n",
      "gt box [40.32 194.986496 390.72 306.773504], anchor [66.98066401624385 141.49033200812192 429.01933598375615 322.5096679918781]\n",
      "iou for anchor [2 2 15 14] and bbox 1 : 0.5281931943636325\n",
      "gt box [40.32 194.986496 390.72 306.773504], anchor [66.98066401624385 157.49033200812192 429.01933598375615 338.5096679918781]\n",
      "iou for anchor [2 2 15 15] and bbox 1 : 0.5281931943636325\n",
      "gt box [40.32 194.986496 390.72 306.773504], anchor [66.98066401624385 173.49033200812192 429.01933598375615 354.5096679918781]\n",
      "iou for anchor [2 2 15 16] and bbox 1 : 0.5281931943636325\n",
      "gt box [40.32 194.986496 390.72 306.773504], anchor [66.98066401624385 189.49033200812192 429.01933598375615 370.5096679918781]\n",
      "iou for anchor [2 2 15 17] and bbox 1 : 0.5281931943636325\n",
      "[0 0 0 0]\n",
      "we found 0 anchors for 0\n",
      "we didnt find any anchor for gt 0\n",
      "we found 0 anchors for 1\n",
      "we didnt find any anchor for gt 1\n",
      "we found 0 anchors for 2\n",
      "we didnt find any anchor for gt 2\n",
      "we found 0 anchors for 3\n",
      "we didnt find any anchor for gt 3\n",
      "found 4 positives and 4268 negatives\n",
      "shape y_is_box_valid (1, 9, 32, 32), nonzero 8\n",
      "shape y_rpn_overlap (1, 9, 32, 32), nonzero 4\n",
      "shape y_rpn_regr (1, 36, 32, 32)\n",
      "shape rpn cls (1, 18, 32, 32)\n",
      "shape rpn regr (1, 72, 32, 32)\n",
      "num pos 4\n"
     ]
    }
   ],
   "source": [
    "batch = get_data_dict(samples, sample_ids, batch_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['image', 'rpn_cls', 'rpn_regr', 'img_data', 'image_id'])\n"
     ]
    }
   ],
   "source": [
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_img = 0    # image index in the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_pos': 4, 'original_size': ('1024', '768'), 'bboxes': [(0.18125, 0.499167, 0.2725, 0.606667), (0.07875, 0.380833, 0.763125, 0.599167), (0.64875, 0.490833, 0.73875, 0.606667), (0.69125, 0.465, 0.725, 0.480833)], 'classes': [(371, 'Wheel'), (63, 'Car'), (371, 'Wheel'), (371, 'Wheel')]}\n"
     ]
    }
   ],
   "source": [
    "print(batch['img_data'][i_img])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original image: height=1024 width=1024\n",
      "Feature map size: height=32 width=32 C.rpn_stride=16\n",
      "(1, 512, 512, 3)\n",
      "Shape of y_rpn_cls (1, 32, 32, 18)\n",
      "Shape of y_rpn_regr (1, 32, 32, 72)\n",
      "[{'num_pos': 4, 'original_size': ('1024', '768'), 'bboxes': [(0.18125, 0.499167, 0.2725, 0.606667), (0.07875, 0.380833, 0.763125, 0.599167), (0.64875, 0.490833, 0.73875, 0.606667), (0.69125, 0.465, 0.725, 0.480833)], 'classes': [(371, 'Wheel'), (63, 'Car'), (371, 'Wheel'), (371, 'Wheel')]}]\n",
      "Number of positive anchors for image 0: 4\n",
      "\n",
      "\n",
      "pos_cls: (array([ 7,  7,  8, 11, 13, 13, 21, 22, 22, 24, 24, 27]), array([17, 17, 10, 24, 14, 14,  6, 17, 17, 15, 15,  9]), array([ 0,  9,  6,  2,  8, 17,  1,  0,  9,  2, 11,  0]))\n",
      "pos_regr: (array([ 7,  7,  7,  7, 13, 13, 13, 13, 22, 22, 22, 22, 24, 24, 24, 24]), array([17, 17, 17, 17, 14, 14, 14, 14, 17, 17, 17, 17, 15, 15, 15, 15]), array([ 0,  1,  2,  3, 32, 33, 34, 35,  0,  1,  2,  3,  8,  9, 10, 11]))\n",
      "y_rpn_cls for possible pos anchor: [1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "y_rpn_regr for positive anchor: [ 1.          1.          1.          1.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      " -0.06        0.048336   -0.31471074 -0.15082289  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "print('Original image: height=%s width=%s' % (batch['img_data'][i_img]['original_size'][0], batch['img_data'][i_img]['original_size'][0]))\n",
    "#print('Resized image:  height=%d width=%d C.im_size=%d'%(X.shape[1], X.shape[2], C.im_size))\n",
    "print('Feature map size: height=%d width=%d C.rpn_stride=%d'%(batch['rpn_cls'][i_img].shape[1], batch['rpn_cls'][i_img].shape[1], RPN_STRIDE))\n",
    "print(batch['image'].shape)\n",
    "print('Shape of y_rpn_cls {}'.format(batch['rpn_cls'][i_img].shape))\n",
    "print('Shape of y_rpn_regr {}'.format(batch['rpn_regr'][i_img].shape))\n",
    "print(batch['img_data'])\n",
    "\n",
    "print('Number of positive anchors for image %s: %d' % (i_img, batch['img_data'][i_img]['num_pos']))\n",
    "\n",
    "print('\\n')\n",
    "cls = batch['rpn_cls'][i_img][0]\n",
    "pos_cls = np.where(cls==1)\n",
    "print('pos_cls: %s' % str(pos_cls))\n",
    "regr = batch['rpn_regr'][i_img][0]\n",
    "pos_regr = np.where(regr==1)\n",
    "print('pos_regr: %s' % str(pos_regr))\n",
    "print('y_rpn_cls for possible pos anchor: {}'.format(cls[pos_cls[0][0],pos_cls[1][0],:]))\n",
    "print('y_rpn_regr for positive anchor: {}'.format(regr[pos_regr[0][0],pos_regr[1][0],:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create figure and axes\n",
    "fig,ax = plt.subplots(1)\n",
    "\n",
    "# Display the image\n",
    "ax.imshow(batch['image'][0])\n",
    "im_width = batch['image'].shape[1]\n",
    "im_height = batch['image'].shape[2]\n",
    "\n",
    "gt_color = (0, 1, 0)\n",
    "anc_color = (0, 0, 1)\n",
    "\n",
    "# Create a Rectangle patch for each GT\n",
    "for i_gt, gt_box in enumerate(batch['img_data'][i_img]['bboxes']):\n",
    "    gt_x1, gt_x2 = gt_box[0] * im_width, gt_box[2] * im_width\n",
    "    gt_y1, gt_y2 = gt_box[1] * im_height, gt_box[3] * im_height\n",
    "    gt_x1, gt_y1, gt_x2, gt_y2 = int(gt_x1), int(gt_y1), int(gt_x2), int(gt_y2)\n",
    "\n",
    "    gt_width = gt_x2 - gt_x1\n",
    "    gt_height = gt_y2 - gt_y1\n",
    "\n",
    "    rect = patches.Rectangle((gt_x1, gt_y1), gt_width, gt_height, linewidth=2, edgecolor=gt_color, facecolor='none')  # bottom left corner\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "# Create rectangles for positive anchors\n",
    "for i_anc in range(batch['img_data'][i_img]['num_pos']):\n",
    "    # color = (100+i*(155/4), 0, 100+i*(155/4))\n",
    "    \n",
    "    idx = pos_regr[2][i_anc*4]/4\n",
    "    anchor_size = ANCHOR_SCALES[int(idx/3)]\n",
    "    anchor_ratio = ANCHOR_RATIOS[2-int((idx+1)%3)]\n",
    "    \n",
    "    center = (pos_regr[1][i_anc*4]*RPN_STRIDE, pos_regr[0][i_anc*4]*RPN_STRIDE)\n",
    "    \n",
    "    anc_w, anc_h = anchor_size*anchor_ratio[0], anchor_size*anchor_ratio[1]\n",
    "    \n",
    "    anc_x1 = center[0]-int(anc_w/2)\n",
    "    anc_y1 = center[1]-int(anc_h/2)\n",
    "    \n",
    "    rect = patches.Rectangle((anc_x1, anc_y1), anc_w, anc_h, linewidth=2, edgecolor=anc_color, facecolor='none')  # bottom left corner\n",
    "    ax.add_patch(rect)\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Original image: height=768 width=1024\n",
    "Resized image:  height=300 width=400 C.im_size=300\n",
    "Feature map size: height=18 width=25 C.rpn_stride=16\n",
    "(1, 300, 400, 3)\n",
    "2 includes 'y_rpn_cls' and 'y_rpn_regr'\n",
    "Shape of y_rpn_cls (1, 18, 25, 18)\n",
    "Shape of y_rpn_regr (1, 18, 25, 72)\n",
    "image_data : {'filepath': 'drive/My Drive/AI/Dataset/Open Images Dataset v4 (Bounding Boxes)/train/1cb71505057b99dc.jpg', 'width': 1024, 'height': 768, 'bboxes': [{'class': 'Car', 'x1': 80, 'x2': 781, 'y1': 292, 'y2': 460}]}\n",
    "Number of positive anchors for this image: 1\n",
    "\n",
    "pos_cls (array([ 9,  9, 15]), array([12, 12, 16]), array([ 5, 14,  5]))\n",
    "pos_regr (array([9, 9, 9, 9]), array([12, 12, 12, 12]), array([20, 21, 22, 23]))\n",
    "y_rpn_cls for possible pos anchor: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
    "y_rpn_regr for positive anchor: [ 0.          0.          0.          0.          0.          0.\n",
    "  0.          0.          0.          0.          0.          0.\n",
    "  0.          0.          0.          0.          0.          0.\n",
    "  0.          0.          1.          1.          1.          1.\n",
    "  0.          0.          0.          0.          0.          0.\n",
    "  0.          0.          0.          0.          0.          0.\n",
    "  0.          0.          0.          0.          0.          0.\n",
    "  0.          0.          0.          0.          0.          0.\n",
    "  0.          0.          0.          0.          0.          0.\n",
    "  0.          0.         -0.7034815  -0.22649515  1.65558708 -1.28599977\n",
    "  0.          0.          0.          0.          0.          0.\n",
    "  0.          0.          0.          0.          0.          0.        ]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_anchor(gt_coords, anc_coords):\n",
    "    img = np.zeros([512,512,3],dtype=np.uint8)\n",
    "    img.fill(255) # or img[:] = 255\n",
    "    \n",
    "    fig,ax = plt.subplots(1)\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(img)\n",
    "    im_width = img.shape[1]\n",
    "    im_height =  img.shape[2]\n",
    "    \n",
    "    gt_width = gt_coords[2] - gt_coords[0]\n",
    "    gt_height = gt_coords[3] - gt_coords[1]\n",
    "    \n",
    "    rect_gt = patches.Rectangle((gt_coords[0], gt_coords[1]), gt_width, gt_height, linewidth=2, edgecolor=gt_color, facecolor='none')\n",
    "    ax.add_patch(rect_gt)\n",
    "    \n",
    "    anc_width = anc_coords[2] - anc_coords[0]\n",
    "    anc_height = anc_coords[3] - anc_coords[1]\n",
    "    \n",
    "    rect_anc = patches.Rectangle((anc_coords[0], anc_coords[1]), anc_width, anc_height, linewidth=2, edgecolor=anc_color, facecolor='none')\n",
    "    ax.add_patch(rect_anc)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_anchor([92.8, 255.573504, 139.52, 310.613504], [337.37258300203047, 250.74516600406096, 382.62741699796953, 341.25483399593907])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /=: 'tuple' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-260-e801a2b98668>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m155\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /=: 'tuple' and 'int'"
     ]
    }
   ],
   "source": [
    "a = (155, 0, 255)\n",
    "a /= 255\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
